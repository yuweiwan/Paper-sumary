{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACL2020调研总结【持续更新】\n",
    "因调研方向原因，目前主要为序列，匹配，分类相关，未包含文本生成，阅读理解，对话系统等内容。<br>\n",
    "根据ACL博客，今年投递论文的情况如下图：<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/qushi.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 趋势\n",
    "结合前几年的统计数据，基本上segmentation, POS tagging等低级任务逐渐减少，而对话，生成等高级任务逐渐增多。信息提取一直是重点，而机器学习类型稳步增长，直到今年的第一（越来越多基于多任务评估的通用模型论文）。更加重视现实场景的应用，同时也对当前的成就和局限性，以及NLP的伦理，道德问题有更多思考。<br>\n",
    "以下为几个具体趋势：\n",
    "\n",
    "### 无监督/弱监督/半监督/远程监督/主动学习/强化学习\n",
    "可以看到研究趋势正远离大型标注文本，转向在无标注文本上进行预训练，然后使用更小的特定于任务/领域的数据集微调。在这次会议上，许多论文都集中在较少监督的训练模型上。<br>\n",
    "**无监督**：<br>\n",
    "已知数据不知道任何标签。<br>\n",
    "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering 尝试从非结构化的知识库中，检索高质量的解释语句：(a) 一种无监督的对齐方法，利用 GloVe 嵌入将问题和答案与解释句子进行软对齐；(b) 一个迭代过程，该迭代过程将重点放在现有解释未覆盖的查询剩余项上；(c) 一个简单的停止条件，当给定问题和候选答案中的所有项都被检索到的解释句子集合覆盖时，该迭代过程结束。AIR 在 MultiRC 上超越了以往的方法，包括有监督的方法。(由于是问答系统，所以未收入表格）<br>\n",
    "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT 提出了一种无监督的探索BERT可解释性的方法——扰动掩码（Perturbed Masking），用于解决到底是BERT本身，还是引入的参数学到了语言学知识的问题。（由于是可解释性，同样未收入表格）<br>\n",
    "**弱监督**：<br>\n",
    "已知数据和其一一对应的弱标签，标签的强弱指的是标签蕴含的信息量的多少。<br>\n",
    "Contextualized Weak Supervision for Text Classification 提出了一个情境化的弱监督框架ConWea，使用了语境化语料库+ Pseudo-Label 的方法，兼容任何文本分类模型，尤其适合精细标签用户定制的文本分类。想法不错适用也广，但具体实践效果有待探讨。<br>\n",
    "Named Entity Recognition without Labelled Data: A Weak Supervision Approach提出了一个收集了多个标签函数的弱监督模型，使用了HMM+emissions进行聚合的方法学习序列标签模型。准确率不是很高，但很适合作为标注辅助工具。<br>\n",
    "**半监督**：<br>\n",
    "已知数据和部分数据一一对应的标签，有一部分数据的标签未知。<br>\n",
    "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification 提出了一种半监督的文本分类学习方法MixText。其关键模型TMix通过在隐藏空间中插入文本来创建大量的增强训练样本。<br>\n",
    "**远程监督**：<br>\n",
    "将已有的知识库（比如 freebase）对应到丰富的非结构化数据中（比如新闻文本），从而生成大量的训练数据，从而训练出一个效果不错的关系抽取器。<br>\n",
    "Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents 研究了由外部知识图约束的噪声标记训练集与人工标注的测试集不一致引起的标签分布移位问题。<br>\n",
    "**主动学习**：<br>\n",
    "学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。<br>\n",
    "Active Learning for Coreference Resolution using Discrete Annotation 为共指消解任务开发一种高效的标注框架，其可通过主动学习来选择最有价值的样本进行标注。（尚未收入表格）<br>\n",
    "**强化学习**：<br>\n",
    "算法在没有人为指导的情况下，通过不断的试错来提升任务性能的过程。<br>\n",
    "Zero-shot Text Classification via Reinforced Self-training 提出了一个强化学习框架来自动学习数据选择策略并提供更可靠的选择。<br>\n",
    "A Reinforced Generation of Adversarial Examples for Neural Machine Translation 利用强化学习建模针对机器翻译生产对抗样本，此方法区别于已有问题解决思路，在没有错误特征建模的前提下，能高效生产特定系统的对抗样本，且可以被进一步用于系统的鲁棒性分析和改善。（由于是机器翻译，未收入表格）<br>\n",
    "\n",
    "### 多语/跨语/多域/跨域/语言迁移/领域迁移\n",
    "**多语/跨语/语言迁移**：<br>\n",
    "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus 关注基于依赖关系的语义角色标注，识别给定谓词的语义角色，适应多语输入。<br>\n",
    "Sources of Transfer in Multilingual Named Entity Recognition 是首个对polyglot-NER模型进行的系统研究，通过对单语数据进行精调，polyglot-NER模型可以适应目标语言，同时解释了这一技术的有效性。<br>\n",
    "Structure-Level Knowledge Distillation For Multilingual Sequence Labeling 提出通过将多个单语模型（教师）的结构知识提取为统一的多语言模型（学生），缩小单语模型与统一多语言模型之间的差距，适用于将单语模型的知识蒸馏用于一个多语模型。<br>\n",
    "Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language 提出了一种师生学习方法用源语言中的NER模型作为教师来训练目标语言中未标记数据的学生模型。该方法适用于单源和多源跨语言NER。<br>\n",
    "Emerging Cross-lingual Structure in Pretrained Language Models 对双语BERT的跨语言表征进行了详细的消融研究，发现参数共享在跨语言表示学习中起着最重要的作用。与单词嵌入（Mikolov et al.，2013）相似，单语BERT可以很容易地与线性映射对齐，从而在每个层次上产生跨语言的表示空间。<br>\n",
    "Hypernymy Detection for Low-Resource Languages via Meta Learning 使用元学习将知识的检测从高资源语言转移到低资源语言。<br>\n",
    "**多域/跨域/领域迁移**：<br>\n",
    "Multi-Cell Compositional LSTM for NER Domain Adaptation 研究了一种用于多任务学习的multi-cell compositional LSTM 结构，使用单独的cell状态对每个实体类型进行建模。为了使源域和目标域在字符级上共享相同的特征空间，在域间使用了一个共享的C单元Cˆ。<br>\n",
    "Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference 提出了一种基于BiLSTM-CRF模型的新体系结构，该模型适用于以下三种数据的实验设置：a）多域；b）在推理时体裁标签未知的多域；c）训练中未遇到的域。<br>\n",
    "Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis 研究了如何有效地将预训练语言模型BERT应用于无监督域自适应。<br>\n",
    "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks 探索了预训练的任务自适应和域自适应，并指出了未来预训练的研究方向：采取更有效的数据选择方法，构建更多有效的无标注数据，将大型LM重构到遥远领域，获得可重用的LM。<br>\n",
    "\n",
    "\n",
    "### 数据不平衡/数据增强/数据标注\n",
    "**数据不平衡**：<br>\n",
    "A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking 提出了adaptive objective，通过在训练过程中动态调整不同slots的权重来缓解slot imbalance。<br>\n",
    "Dice Loss for Data-imbalanced NLP Tasks 提出一个基于Dice Loss的自适应损失DSC，在训练时推动模型更加关注困难的样本，降低简单负例的学习度，从而在整体上提高基于F1值的效果。<br>\n",
    "**数据增强**：<br>\n",
    "Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation 中模型生成一个新的句子，同时保留原有的opinion targets和labels。<br>\n",
    "Good-Enough Compositional Data Augmentation 提出使用在相似上下文中更常见的短语来替代罕见短语，从而提升神经网络的组成泛化性能，该协议与模型无关，可用于各种任务。<br>\n",
    "NAT: Noise-Aware Training for Robust Neural Sequence Labeling 提出了两个噪声感知训练（NAT）目标，以提高序列标记对扰动输入的鲁棒性。<br>\n",
    "**数据标注**：<br>\n",
    "From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains 提出了一种新的领域无关的Human-In-The-Loop方法：使用recommenders建议潜在的概念和自适应候选排序，从而加快了整个注释过程，适用于低资源领域的数据标注，为创建标注提供推荐。<br>\n",
    "\n",
    "\n",
    "### 语言模型的补充（这一部分参考了https://zhuanlan.zhihu.com/p/163713448 ）\n",
    "我们已经知道，来自语言模型的知识是缺乏和不准确的。例如语言模型对否定不敏感，因此很容易因错误指定的探针或相关但不准确的答案造成混淆。\n",
    "当前正被采用的多种不同方案：（未收入表格）<br>\n",
    "**检索**：<br>\n",
    "Repl4NLP 研讨会上提到了检索式增强的语言模型。<br>\n",
    "Kristina Toutanova 谈到了谷歌的 REALM (出自REALM: Retrieval-Augmented Language Model Pre-Training）以及使用有关实体的知识来增强语言模型。<br>\n",
    "Mike Lewis 谈到了最近邻语言模型（出自Generalization through Memorization: Nearest Neighbor Language Models），其可以改善对事实知识的预测结果；另外他还谈到了 Facebook 的 RAG 模型（出自Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks），其将一个生成器和一个检索组件组合到了一起。<br>\n",
    "**使用外部知识库**：<br>\n",
    "多年来的常见做法。\n",
    "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation 使用来自常识知识库的知识增强了 GPT-2 模型处理常识任务的能力。<br>\n",
    "Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness 使用了这样的知识库来进行对话生成。<br>\n",
    "**让语言模型具备新能力**：\n",
    "Temporal Common Sense Acquisition with Minimal Supervision 训练一个语言模型来获取时间知识（比如时间的频率和持续时间），其使用了通过模式和 SRL 的信息提取得到的训练实例。<br>\n",
    "Injecting Numerical Reasoning Skills into Language Models 通过对使用模板生成的数字数据和需要对数字进行推理的文本数据进行微调来将数字技能注入BERT。<br>\n",
    "\n",
    "### 成就 & 局限反思以及对 NLP 未来的思考（这一部分参考了https://zhuanlan.zhihu.com/p/163713448 和https://www.jiqizhixin.com/articles/2020-08-06-2 ）（大部分未收入表格）\n",
    "**关于数据与模型的关系**：<br>\n",
    "Tal Linzen 的荣誉提名主题论文 How Can We Accelerate Progress Towards Human-like Linguistic Generalization?认为我们用大量数据学习的模型在面对人类可以获得的数据量时，可能学不到任何东西，而这些模型在数据中找到的统计学模式可能在人类看来根本无关紧要。他建议说未来我们应该标准化大小适中的预训练语料库，使用专家创建的评估集以及奖励成功的少量次学习。<br>\n",
    "Kathy McKeown 的主题演讲也提到了这一点，并补充说，排行榜并不总是能帮助该领域的发展。基准通常只体现了分布的头部，但我们还需要看到分布的尾部。此外，分析通用模型（比如语言模型）在具体任务上的进展是很困难的。<br>\n",
    "Bonnie Webber 在她的终身成就奖访谈中强调，我们需要检查数据和分析模型错误。即便只是简单地看看精度和召回率，而不仅仅是聚合 F1 分数，就能帮助我们理解模型的弱点和优势。虽然神经网络能够解决不需要深度理解的任务，但更具挑战性的目标是识别隐含的含义和世界知识。<br>\n",
    "Yanaka et al. （出自Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?）和 Goodwin et al. （出自Probing Linguistic Systematicity）都指出神经自然语言理解模型缺乏系统性，几乎不能泛化已学习到的语义现象。<br>\n",
    "Emily Bender 和 Alexander Koller 的最佳主题论文Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data认为仅从形式学习含义是不可能的。<br>\n",
    "**关于歧义和不确定性**：<br>\n",
    "Ellie Pavlick 在 Repl4NLP 的演讲探讨了在清晰定义语义研究中的目标方面的困难。简单地将语义理论翻译成 NLI 风格的任务注定要失败，因为语言植根于更广阔的语境之中。<br>\n",
    "Guy Emerson （出自What are the Goals of Distributional Semantics?）定义了分布的语义所需的性质，其中之一是学习不确定性。<br>\n",
    "Feng et al. （出自“None of the Above”: Measure Uncertainty in Dialog Response Retrieval）设计了一种对话响应任务和包含一个「none of the above（以上皆不对）」回应的模型。<br>\n",
    "最后，Trott et al. （出自(Re)construing Meaning in NLP）指出尽管语义任务可用于识别两个具有同样含义的表达，但也可用于识别表述上的差异会如何影响含义。<br>\n",
    "Gonen et al. （出自Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora）提出了一种用于测量词义变化的非常直观和可解释的方法，其具体做法为检查词分布的最近邻。<br>\n",
    "**关于热度很高的预训练模型**：<br>\n",
    "肯定态度：Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks，Pretrained Transformers Improve Out-of-Distribution Robustness，Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning等文章都认为预训练具有大量数据所积累的泛化能力，可以很好地实现领域，不同数据和任务的迁移/特化。<br>\n",
    "否定态度：To Pretrain or Not to Pretrain: Examining the Benefits of Pretraining on Resource Rich Tasks和Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly则认为预训练模型对语言的知识、逻辑理解有限。后者发现当前的预训练模型很容易被干扰，例如句子中的否定和“错误”。它是通过较浅层的模式匹配的方式解决开放域QA问题，而不是基于事实知识和推理。<br>\n",
    "探索态度：Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?则进行了大规模实验，观察到，需要高级推理能力的中间任务，对于提升目标任务的效果往往更好，而经过中间任务的学习后可能导致模型忘记一些预训练中得到的知识。Do You Have the Right Scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods提出一种称为 MC-Tailor 的方法，把高估区域的概率质量（probalility mass）截断和转移到低估区域。在文本生成任务上，MC-Tailor 效果显著优于微调方法。这表明在小数据集上应用预训练模型，微调可能不是拟合数据的最好方法。<br>\n",
    "安全问题：Weight Poisoning Attacks on Pretrained Models 认为预训练模型在微调后可能会暴露“后门”。作者通过一种 RIPPLe 的正则化方法和 Embedding Surgery 的初始化方法，构建 weight poisoning 攻击，甚至可能构建高达 100% 成功率的后门。<br>\n",
    "\n",
    "### 总结\n",
    "这是一个对过去的成果不断融合以提高的时代，将有缺点的模型融合改进，追求更好，更快，同时也追求更科学的数据集建立与评估，普适性（低资源的迁移），关心特殊性（特定语言结构，特定领域），预防危险性（盲目乐观，伦理，安全性）。预训练模型充分证明了我们可以从海量的无标注文本中学到大量潜在的知识，而无需为每一项自然语言处理任务都标注大量的数据。深度学习让多模态信息的处理和融合进一步发展。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
